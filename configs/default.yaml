# Default configuration for RAG Lab experiments

# Generator configuration (optimized for speed)
generator:
  model_name: "Qwen/Qwen2.5-1.5B-Instruct"
  max_new_tokens: 16  # Covers 100% of SQuAD answers (max 9 tokens needed)
  temperature: 0.0
  top_p: 1.0
  seed: 42
  context_token_budget: 512  # Reduced from 1024 for speed
  stop_sequences: []
  device:
    device: "cpu"  # Use CPU due to MPS memory limitations
    torch_dtype: "float16"  # Use float16 for speed

# Dataset configuration
dataset:
  name: "rajpurkar/squad"
  split: "validation"
  version: "1.1"
  cache_dir: null

# Evaluation configuration
evaluation:
  metrics: ["exact_match", "f1"]
  retrieval_metrics: ["recall@k", "mrr@k", "ndcg@k"]
  k_values: [1, 3, 5, 10]
  save_predictions: true
  save_plots: true

# Output configuration
output:
  base_dir: "runs"
  artifacts_dir: "artifacts"
  indexes_dir: "artifacts/indexes"
  logs_dir: "logs"

# Retriever configurations
boolean:
  k: 5
  chunk_size: 512
  chunk_stride: 256
  max_chars: null
  normalize_tokens: true
  case_sensitive: false

tfidf:
  k: 5
  chunk_size: 512
  chunk_stride: 256
  max_chars: null
  ngram_range: [1, 2]
  min_df: 2
  max_df: 1.0
  norm: "l2"

bm25:
  k: 5
  chunk_size: 512
  chunk_stride: 256
  max_chars: null
  k1: 1.5
  b: 0.75

dense:
  k: 5
  chunk_size: 512
  chunk_stride: 256
  max_chars: null
  model_name: "BAAI/bge-small-en-v1.5"
  normalize_embeddings: true
  faiss_index_type: "IndexFlatIP"
  batch_size: 32

sota:
  k: 5
  chunk_size: 512
  chunk_stride: 256
  max_chars: null
  model_name: "BAAI/bge-small-en-v1.5"
  normalize_embeddings: true
  faiss_index_type: "IndexFlatIP"
  batch_size: 32
  k_rerank: 50
  cross_encoder_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  rerank_batch_size: 16

# Logging
log_level: "INFO"
log_file: null

# Reproducibility
seed: 42
